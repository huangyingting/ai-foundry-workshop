{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d8f7b1",
      "metadata": {},
      "source": [
        "# üöÄ DeepSeek-R1 Model with Azure AI Inference üß†\n",
        "\n",
        "**DeepSeek-R1** is a state-of-the-art reasoning model combining reinforcement learning and supervised fine-tuning, excelling at complex reasoning tasks with 37B active parameters and 128K context window.\n",
        "\n",
        "In this notebook, you'll learn to:\n",
        "1. **Initialize** the ChatCompletionsClient for Azure serverless endpoints\n",
        "2. **Chat** with DeepSeek-R1 using reasoning extraction\n",
        "3. **Implement** a travel planning example with step-by-step reasoning\n",
        "4. **Leverage** the 128K context window for complex scenarios\n",
        "\n",
        "## Why DeepSeek-R1?\n",
        "- **Advanced Reasoning**: Specializes in chain-of-thought problem solving\n",
        "- **Massive Context**: 128K token window for detailed analysis\n",
        "- **Efficient Architecture**: 37B active parameters from 671B total\n",
        "- **Safety Integrated**: Built-in content filtering capabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e3a4c2",
      "metadata": {},
      "source": [
        "## 1. Setup & Authentication\n",
        "\n",
        "Required packages:\n",
        "- `azure-ai-inference`: For chat completions\n",
        "- `python-dotenv`: For environment variables\n",
        "\n",
        ".env file requirements:\n",
        "```bash\n",
        "AZURE_INFERENCE_ENDPOINT=<your-endpoint-url>\n",
        "AZURE_INFERENCE_KEY=<your-api-key>\n",
        "MODEL_NAME=DeepSeek-R1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53f8d4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\",\n",
        "                     \"https://aaisc.services.ai.azure.com/models\")\n",
        "model_name = os.getenv(\"MODEL_NAME\", \"DeepSeek-R1\")\n",
        "\n",
        "# Initialize client\n",
        "try:\n",
        "  client = ChatCompletionsClient(endpoint=endpoint, credential=DefaultAzureCredential(\n",
        "  ), credential_scopes=[\"https://cognitiveservices.azure.com/.default\"])\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"‚ùå Initialization failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c01d5d9",
      "metadata": {},
      "source": [
        "## 2. Intelligent Travel Planning ‚úàÔ∏è\n",
        "\n",
        "Demonstrate DeepSeek-R1's reasoning capabilities for trip planning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e6a5d8d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üó∫Ô∏è Query: Plan a 5-day cultural trip to Kyoto in April\n",
            "\n",
            "üß† Thinking Process: Okay, the user wants a 5-day cultural trip to Kyoto in April, including hidden gems and safety tips. Let me start by recalling Kyoto's main attractions and the best times to visit them. April is cherry blossom season, so some spots might be crowded. They mentioned hidden gems, so I need to include less touristy places. Also, safety considerations are important, especially regarding crowds, weather, and COVID-19 measures.\n",
            "\n",
            "First, day one: Arrival. Kiyomizu-dera is a must, but maybe suggest going early to avoid crowds. Then Sannenzaka and Ninenzaka for traditional streets. Lunch in Gion, maybe a specific restaurant. Hidden gem: Ishibei-koji Lane. Evening in Gion, but mention being respectful in geisha areas. Safety tip: wear comfortable shoes for the stone paths.\n",
            "\n",
            "Day two: Arashiyama. Bamboo Grove is popular, so early morning. Then Tenryu-ji. Hidden gem: Otagi Nenbutsu-ji Temple with the statues. Lunch in Arashiyama. Afternoon: Monkey Park Iwatayama. Safety: watch for monkeys. Maybe a river cruise. Evening in Pontocho Alley, but note that some places might need reservations.\n",
            "\n",
            "Day three: Northern Kyoto. Ryoan-ji and Kinkaku-ji (Golden Pavilion). Both can be crowded. Hidden gem: Hirano Shrine for cherry blossoms. Lunch near Kitano Tenmangu. Afternoon: Kamishichiken geisha district. Tea ceremony experience. Safety: April weather can be unpredictable, so bring layers.\n",
            "\n",
            "Day four: Day trip to Uji. Byodo-in Temple, Uji River, tea tasting. Hidden gem: Mimuroto-ji Temple for flowers. Lunch with Uji noodles. Return to Kyoto, evening in Fushimi Inari. Safety: Fushimi Inari at night is okay but bring a flashlight.\n",
            "\n",
            "Day five: Eastern Kyoto. Nanzen-ji Temple, Philosopher's Path. Hidden gem: Honen-in Temple. Lunch in Okazaki area. Afternoon: Kyoto Imperial Palace. Hidden gem: Demachi Masugata Shopping Arcade. Final dinner in Ponto-cho or Kiyamachi.\n",
            "\n",
            "Safety considerations: COVID-19 measures, crowded places, weather (layers, rain gear), pickpocketing in crowded areas, food allergies, emergency numbers. Also, maybe suggest a prepaid transport card.\n",
            "\n",
            "Wait, did I miss any hidden gems? Let me check each day again. Ishibei-koji, Otagi Nenbutsu-ji, Hirano Shrine, Mimuroto-ji, Honen-in, Demachi Masugata. That seems good. Also, safety tips for each day, like monkeys in Arashiyama, stone paths in Gion, night visits to Fushimi Inari.\n",
            "\n",
            "Transportation: Buses and trains. Maybe mention IC cards. Also, language tips, though in Kyoto some English is spoken. Cash is important for smaller shops.\n",
            "\n",
            "April-specific: Cherry blossoms, so hanami spots but also alternatives if too crowded. Maybe mention the possibility of rain and carrying an umbrella.\n",
            "\n",
            "I think that covers the main points. Need to structure each day with morning, afternoon, evening, hidden gems, and safety. Make sure the plan is logical in terms of location to minimize travel time. For example, grouping nearby attractions each day. Also, balance between cultural sites, nature, and local experiences like tea ceremonies and food.\n",
            "\n",
            "üìù Final Answer: **5-Day Cultural Trip to Kyoto in April: Hidden Gems & Safety Tips**  \n",
            "*Rationale: April offers mild weather and cherry blossoms (sakura), but crowds peak. This itinerary balances iconic sites with lesser-known spots and prioritizes safety.*  \n",
            "\n",
            "---\n",
            "\n",
            "### **Day 1: Historic Higashiyama & Gion**  \n",
            "**Morning:**  \n",
            "- **Kiyomizu-dera Temple** (6:00 AM arrival to avoid crowds).  \n",
            "  *Hidden Gem:* Explore **Jishu Shrine** (love stones) and the quieter **Otowa Waterfall** paths.  \n",
            "- Walk **Sannenzaka & Ninenzaka** (traditional streets; avoid midday crowds).  \n",
            "\n",
            "**Afternoon:**  \n",
            "- Lunch at **Gion Kinana** (matcha desserts) or **Kagizen Yoshifusa** (traditional sweets).  \n",
            "- **Kodai-ji Temple** (bamboo groves and zen gardens).  \n",
            "  *Hidden Gem:* **Ishibei-koji Lane** (near Kodai-ji), a secluded stone-paved alley with teahouses.  \n",
            "\n",
            "**Evening:**  \n",
            "- Stroll **Gion** at dusk for geiko/maiko sightings.  \n",
            "  *Safety:* Respect privacy; avoid intrusive photography.  \n",
            "\n",
            "---\n",
            "\n",
            "### **Day 2: Arashiyama & Sagano**  \n",
            "**Morning:**  \n",
            "- **Arashiyama Bamboo Grove** (arrive by 7\n"
          ]
        }
      ],
      "source": [
        "def plan_trip_with_reasoning(query, show_thinking=False):\n",
        "  \"\"\"Get travel recommendations with reasoning extraction\"\"\"\n",
        "  messages = [\n",
        "      SystemMessage(\n",
        "          content=\"You are a travel expert. Provide detailed plans with rationale.\"),\n",
        "      UserMessage(\n",
        "          content=f\"{query} Include hidden gems and safety considerations.\")\n",
        "  ]\n",
        "\n",
        "  response = client.complete(\n",
        "      messages=messages,\n",
        "      model=model_name,\n",
        "      temperature=0.7,\n",
        "      max_tokens=1024\n",
        "  )\n",
        "\n",
        "  content = response.choices[0].message.content\n",
        "\n",
        "  # Extract reasoning if present\n",
        "  if show_thinking:\n",
        "    match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "    if match:\n",
        "      return {\"thinking\": match.group(1).strip(), \"answer\": match.group(2).strip()}\n",
        "  return content\n",
        "\n",
        "\n",
        "# Example usage\n",
        "query = \"Plan a 5-day cultural trip to Kyoto in April\"\n",
        "result = plan_trip_with_reasoning(query, show_thinking=True)\n",
        "\n",
        "print(\"üó∫Ô∏è Query:\", query)\n",
        "if isinstance(result, dict):\n",
        "  print(\"\\nüß† Thinking Process:\", result[\"thinking\"])\n",
        "  print(\"\\nüìù Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "  print(\"\\nüìù Response:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f1b3a",
      "metadata": {},
      "source": [
        "## 3. Technical Problem Solving üíª\n",
        "\n",
        "Showcase coding/optimization capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e5d4a3e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Problem: How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
            "Consider indexing strategies, hardware requirements, and query optimization.\n",
            "\n",
            "‚öôÔ∏è Solution: <think>\n",
            "Okay, so I need to figure out how to optimize a PostgreSQL database that's handling 10k transactions per second. That's a pretty high load, so I need to make sure everything is tuned properly. Let me start by breaking down the problem into parts: indexing strategies, hardware requirements, and query optimization. Maybe there are other areas too, like configuration settings or replication for scaling.\n",
            "\n",
            "First, indexing. Indexes are crucial for speeding up queries, but they can also slow down writes if overused. For 10k TPS, the right indexes are essential. I should consider the types of indexes. B-tree is the default and good for most cases. Maybe using partial indexes if some queries filter on a specific condition often. Also, covering indexes (INCLUDE) to include columns that are frequently accessed but not part of the WHERE clause. Composite indexes for queries that filter on multiple columns. But need to be careful with the order of columns in composite indexes. Also, maybe BRIN indexes for large tables with naturally ordered data, like timestamps. But BRIN is more efficient for very large tables. Also, avoiding too many indexes on tables that are frequently written to, since each index adds overhead on inserts, updates, deletes.\n",
            "\n",
            "Then, hardware. For 10k TPS, the hardware must be robust. Storage is a big factor. SSDs are a must, preferably NVMe for lower latency. Enough RAM to fit the working set into memory. PostgreSQL's shared_buffers should be set to a portion of the total RAM, maybe 25%, but depends on the total memory. Also, checkpoints and WAL settings. Having a RAID configuration or using a storage solution with redundancy and high IOPS. CPU cores: PostgreSQL is process-based, so multiple cores can help with parallel queries. Maybe using connection pooling to handle the high number of connections without overloading the system. Connection poolers like PgBouncer can help manage connections efficiently.\n",
            "\n",
            "Query optimization. Making sure that all queries are using indexes properly. Using EXPLAIN ANALYZE to check query plans. Avoiding N+1 queries in the application code. Maybe rewriting complex queries, using joins instead of multiple subqueries. Batch processing where possible. Normalization vs denormalization: if some tables are heavily read, maybe denormalize a bit to reduce joins. Also, using prepared statements to reduce parsing overhead. Maybe partitioning large tables to split them into smaller chunks, which can help with index size and query performance. For example, range partitioning on a date column if data is time-based.\n",
            "\n",
            "Configuration tuning. The postgresql.conf settings. Increasing max_connections if needed, but better to use a connection pool. shared_buffers, work_mem, maintenance_work_mem. Autovacuum settings: ensuring that autovacuum is properly tuned to handle dead tuples without causing bloat. Checkpoint_segments and checkpoint_timeout to spread out checkpoints and avoid IO spikes. Synchronous_commit: maybe turning it off for some non-critical transactions to reduce write latency. But that's a trade-off with durability.\n",
            "\n",
            "Replication and scaling. Maybe using read replicas to distribute read queries. If the workload is read-heavy, offloading reads to replicas. For writes, maybe sharding, but PostgreSQL doesn't have built-in sharding, so would need something like Citus or application-level sharding. But sharding adds complexity. Alternatively, using partitioning as mentioned before.\n",
            "\n",
            "Monitoring and maintenance. Tools like pg_stat_statements to identify slow queries. Regular vacuuming and analyzing. Logging slow queries and optimizing them. Using monitoring tools to track performance metrics like cache hit ratio, locks, replication lag if applicable.\n",
            "\n",
            "Let me think about each area in more detail.\n",
            "\n",
            "Indexing strategies: For high TPS, the indexes should be as efficient as possible. For example, using indexes on columns that are frequently used in WHERE clauses, joins, or ORDER BY. Also, using UNIQUE indexes where applicable to enforce constraints efficiently. Maybe considering hash indexes for equality checks if using PostgreSQL 10+, but hash indexes aren't crash-safe until a certain version. Index-only scans can be beneficial if the index includes all columns needed by the query. So using INCLUDE clause in indexes to cover more queries. Also, regularly reindexing or using CONCURRENTLY to rebuild indexes without locking tables, but that's more for maintenance.\n",
            "\n",
            "Hardware: CPU needs to handle 10k transactions. Let's say each transaction is a simple query, but 10k per second is 10,000 TPS. If each transaction is a write, that's a lot. So storage must handle the write load. WAL writes are sequential, so having a separate disk for WAL could help. Also, sufficient RAM to cache frequently accessed data. The working set (frequently accessed data) should fit into memory to avoid disk I/O. For example, if the database is 100GB, having 64GB RAM might not be enough, but if the active data is 32GB, then shared_buffers set to 16GB and the OS cache can handle the rest. But exact numbers depend on the data.\n",
            "\n",
            "Query optimization: Ensuring that all are using indexes. Sometimes, ORM queries can generate suboptimal SQL. So analyzing the actual queries, using slow query logs. Also, avoiding SELECT *, especially if there are large columns. Using LIMIT where applicable. Maybe materialized views for complex aggregations that don't need real-time data. But materialized views need to be refreshed, so it's a trade-off. Also, using appropriate isolation levels to reduce locking. For example, using READ COMMITTED instead of SERIALIZABLE if possible.\n",
            "\n",
            "Configuration: Adjusting parameters like effective_cache_size, random_page_cost (if using SSDs, set to lower value). max_worker_processes for parallel queries. Enabling parallel queries where beneficial. Also, adjusting autovacuum parameters: autovacuum_vacuum_scale_factor, autovacuum_analyze_scale_factor. Maybe lowering the scale factor for large tables to vacuum more aggressively. Increasing autovacuum_max_workers if there are many tables.\n",
            "\n",
            "Replication: If the writes are 10k TPS, a single master might be a bottleneck. So maybe considering synchronous replication for high availability, but that can add latency. Alternatively, using a distributed system. But scaling writes is harder. Maybe application-level partitioning (sharding) to split data across multiple PostgreSQL instances. Each shard handles a subset of the data, reducing the load per instance.\n",
            "\n",
            "Other considerations: Using connection pooling to prevent too many connections from overwhelming the database. Each connection in PostgreSQL is a process, so having thousands of connections can cause overhead. PgBouncer in transaction mode can pool connections and reuse them. Also, prepared statements can help with frequently executed queries by reducing parsing time.\n",
            "\n",
            "Monitoring: Tools like Prometheus with Grafana for visualizing metrics. pgBadger for analyzing logs. Setting up alerts for long-running transactions, lock waits, or replication delays. Regular health checks.\n",
            "\n",
            "So putting it all together, the optimization would involve a combination of proper indexing, scaling hardware appropriately, tuning configuration settings, optimizing queries, using connection pooling, partitioning or sharding, and setting up replication for read scaling. Also, ongoing monitoring to identify bottlenecks as they arise.\n",
            "</think>\n",
            "\n",
            "To optimize a PostgreSQL database handling **10k transactions/second**, implement the following strategies across indexing, hardware, query optimization, configuration, and scaling:\n",
            "\n",
            "### **1. Indexing Strategies**\n",
            "- **B-Tree Indexes**: Optimize for common query patterns (WHERE, JOIN, ORDER BY).\n",
            "- **Partial Indexes**: For queries targeting subsets of data (e.g., `WHERE status = 'active'`).\n",
            "- **Composite Indexes**: Order columns by selectivity and query patterns.\n",
            "- **Covering Indexes**: Use `INCLUDE` to add frequently accessed columns and enable index-only scans.\n",
            "- **BRIN Indexes**: For large time-series tables with ordered data (e.g., timestamps).\n",
            "- **Avoid Over-Indexing**: Balance read/write performance; rebuild indexes concurrently (`REINDEX CONCURRENTLY`).\n",
            "\n",
            "### **2. Hardware Requirements**\n",
            "- **Storage**: NVMe SSDs for low-latency I/O; separate disks for WAL and data.\n",
            "- **RAM**: Size to fit the active dataset (e.g., 64GB+ with 25-40% allocated to `shared_buffers`).\n",
            "- **CPU**: High-core count (16+ cores) to handle parallel queries and connections.\n",
            "- **Network**: Low-latency, high-throughput connections (10Gbps+).\n",
            "\n",
            "### **3. Query Optimization**\n",
            "- **Analyze Execution Plans**: Use `EXPLAIN (ANALYZE, BUFFERS)` to identify bottlenecks.\n",
            "- **Batch Operations**: Reduce round-trips with bulk inserts/updates.\n",
            "- **Avoid N+1 Queries**: Use joins or CTEs instead of iterative application-layer queries.\n",
            "- **Parameterized Queries**: Use prepared statements to reduce parsing overhead.\n",
            "- **Limit Result Sets**: Avoid `SELECT *`; use pagination (`LIMIT/OFFSET` or keyset pagination).\n",
            "\n",
            "### **4. PostgreSQL Configuration**\n",
            "- **Memory Settings**:\n",
            "  - `shared_buffers = 25-40% of RAM`\n",
            "  - `work_mem = 4-32MB` (adjust based on concurrent sorts/hashes).\n",
            "  - `effective_cache_size = 50-75% of total RAM`.\n",
            "- **Autovacuum**: Tune `autovacuum_vacuum_scale_factor` and `autovacuum_max_workers` to prevent bloat.\n",
            "- **Checkpoints**: Reduce frequency with `max_wal_size` and `checkpoint_timeout`.\n",
            "- **Parallel Query**: Enable `max_parallel_workers` and `parallel_tuple_cost` tuning.\n",
            "- **Synchronous Commit**: Disable (`synchronous_commit = off\n"
          ]
        }
      ],
      "source": [
        "def solve_technical_problem(problem):\n",
        "  \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "  response = client.complete(\n",
        "      messages=[\n",
        "          UserMessage(\n",
        "              content=f\"{problem} Please reason step by step, and put your final answer within \\boxed{{}}.\")\n",
        "      ],\n",
        "      model=model_name,\n",
        "      temperature=0.3,\n",
        "      max_tokens=2048\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# Database optimization example\n",
        "problem = \"\"\"How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
        "Consider indexing strategies, hardware requirements, and query optimization.\"\"\"\n",
        "\n",
        "print(\"üîß Problem:\", problem)\n",
        "print(\"\\n‚öôÔ∏è Solution:\", solve_technical_problem(problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9f7a8c",
      "metadata": {},
      "source": [
        "## 4. Best Practices & Considerations\n",
        "\n",
        "1. **Reasoning Handling**: Use regex to separate <think> content from final answers\n",
        "2. **Safety**: Built-in content filtering - handle HttpResponseError for violations\n",
        "3. **Performance**:\n",
        "   - Max tokens: 4096\n",
        "   - Rate limit: 200K tokens/minute\n",
        "4. **Cost**: Pay-as-you-go with serverless deployment\n",
        "5. **Streaming**: Implement response streaming for long completions\n",
        "\n",
        "```python\n",
        "# Streaming example\n",
        "response = client.complete(..., stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
        "```\n",
        "\n",
        "## üéØ Key Takeaways\n",
        "- Leverage 128K context for detailed analysis\n",
        "- Extract reasoning steps for debugging/analysis\n",
        "- Combine with Azure AI Content Safety for production\n",
        "- Monitor token usage via response.usage\n",
        "\n",
        "> Always validate model outputs for critical applications!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
